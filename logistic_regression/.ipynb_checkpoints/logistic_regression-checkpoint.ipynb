{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on binary data\n",
    "\n",
    "Recall that logistic regression has the form: **1 / (1 + e^z) where z = (b) + (w1)(x1) + ... (wn)(xn)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bias = 0\n",
    "        self.theta = None\n",
    "        self.lr = 0.001\n",
    "        self.epochs = 1000\n",
    "        self.decision_threshold = 0.5\n",
    "        \n",
    "    def split_train_test(self):\n",
    "        y = self.df.as_matrix(columns=[self.target_feature])\n",
    "        X = self.df.drop(self.target_feature, axis=1).as_matrix()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                            test_size=self.test_size, random_state=42)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def cost_function(self):\n",
    "        preds = self.predict(self.X_train)\n",
    "        error = (self.y_train * np.log(preds)) + ((1 - self.y_train) * np.log(1 - preds))\n",
    "        return -error.sum() / len(self.X_train)\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return (1 / (1 + np.exp(-z)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.theta))\n",
    "        \n",
    "    def gradient_descent(self):\n",
    "        preds = self.predict(self.X_train)\n",
    "        error = preds - self.y_train\n",
    "        gradient = np.dot(np.transpose(self.X_train), error)\n",
    "        gradient /= len(self.X_train)\n",
    "        gradient *= self.lr\n",
    "        self.theta -= gradient\n",
    "        \n",
    "    def decision_threshold(self, y):\n",
    "        return 1 if y >= self.decision_threshold else 0\n",
    "    \n",
    "    def classify(self, X):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def train(self, df, target_feature, decision_threshold=0.5, test_size=0.2, learning_rate=0.01, epochs=100):\n",
    "        self.df = df\n",
    "        self.decision_threshold = decision_threshold\n",
    "        self.test_size = test_size\n",
    "        self.target_feature = target_feature\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.theta = np.zeros((len(df.columns) - 1, 1), dtype=np.float32)\n",
    "        self.split_train_test()\n",
    "        for i in range(0, self.epochs):\n",
    "            self.gradient_descent()\n",
    "            print(\"Loss:\", self.cost_function())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    681\n",
      "6    638\n",
      "7    199\n",
      "4     53\n",
      "8     18\n",
      "3     10\n",
      "Name: quality, dtype: int64 \n",
      "\n",
      "     pH  alcohol  quality\n",
      "0  3.51      9.4        0\n",
      "1  3.20      9.8        0\n",
      "2  3.26      9.8        0\n",
      "3  3.16      9.8        1\n",
      "4  3.51      9.4        0 \n",
      "\n",
      "Loss: 0.6929991547310735\n",
      "Loss: 0.6929111537444642\n",
      "Loss: 0.6928532619561972\n",
      "Loss: 0.6928104770444732\n",
      "Loss: 0.6927752754033148\n",
      "Loss: 0.6927438836653939\n",
      "Loss: 0.6927144091840419\n",
      "Loss: 0.6926859025858222\n",
      "Loss: 0.6926578876033251\n",
      "Loss: 0.6926301253122141\n",
      "Loss: 0.6926024958408306\n",
      "Loss: 0.6925749390597307\n",
      "Loss: 0.6925474247935092\n",
      "Loss: 0.6925199378707199\n",
      "Loss: 0.6924924707290985\n",
      "Loss: 0.6924650195405884\n",
      "Loss: 0.6924375823931092\n",
      "Loss: 0.6924101583126226\n",
      "Loss: 0.6923827468121357\n",
      "Loss: 0.6923553476556973\n",
      "Loss: 0.6923279607052336\n",
      "Loss: 0.6923005859072454\n",
      "Loss: 0.692273223203863\n",
      "Loss: 0.6922458726078888\n",
      "Loss: 0.692218534052508\n",
      "Loss: 0.6921912075700503\n",
      "Loss: 0.692163893175009\n",
      "Loss: 0.6921365908159994\n",
      "Loss: 0.6921093004970315\n",
      "Loss: 0.6920820222601552\n",
      "Loss: 0.6920547560163633\n",
      "Loss: 0.6920275018285077\n",
      "Loss: 0.6920002596510759\n",
      "Loss: 0.6919730295135071\n",
      "Loss: 0.6919458113434274\n",
      "Loss: 0.6919186051535978\n",
      "Loss: 0.6918914110436729\n",
      "Loss: 0.6918642289301707\n",
      "Loss: 0.6918370588165482\n",
      "Loss: 0.6918099006064318\n",
      "Loss: 0.6917827543920153\n",
      "Loss: 0.6917556201693336\n",
      "Loss: 0.6917284979381272\n",
      "Loss: 0.6917013876981338\n",
      "Loss: 0.6916742893493739\n",
      "Loss: 0.691647202980231\n",
      "Loss: 0.6916201285904416\n",
      "Loss: 0.6915930661760393\n",
      "Loss: 0.6915660156408316\n",
      "Loss: 0.6915389770694206\n",
      "Loss: 0.6915119503730551\n",
      "Loss: 0.6914849356325993\n",
      "Loss: 0.6914579328440913\n",
      "Loss: 0.6914309419188381\n",
      "Loss: 0.6914039629376489\n",
      "Loss: 0.6913769958965641\n",
      "Loss: 0.6913500407069479\n",
      "Loss: 0.6913230974495557\n",
      "Loss: 0.691296166028406\n",
      "Loss: 0.6912692465316049\n",
      "Loss: 0.6912423389625764\n",
      "Loss: 0.6912154431260554\n",
      "Loss: 0.6911885592940334\n",
      "Loss: 0.6911616872786513\n",
      "Loss: 0.6911348270760151\n",
      "Loss: 0.6911079786896038\n",
      "Loss: 0.6910811422844995\n",
      "Loss: 0.6910543176877506\n",
      "Loss: 0.691027504888093\n",
      "Loss: 0.6910007038816355\n",
      "Loss: 0.6909739146644873\n",
      "Loss: 0.6909471372327586\n",
      "Loss: 0.6909203717660379\n",
      "Loss: 0.6908936180695177\n",
      "Loss: 0.6908668761613879\n",
      "Loss: 0.690840146015682\n",
      "Loss: 0.6908134276285175\n",
      "Loss: 0.6907867210107188\n",
      "Loss: 0.6907600263269273\n",
      "Loss: 0.6907333434046458\n",
      "Loss: 0.6907066722252875\n",
      "Loss: 0.690680012799666\n",
      "Loss: 0.6906533651092023\n",
      "Loss: 0.690626729150018\n",
      "Loss: 0.6906001049329188\n",
      "Loss: 0.6905734924393366\n",
      "Loss: 0.6905468918483076\n",
      "Loss: 0.6905203029876358\n",
      "Loss: 0.6904937258534356\n",
      "Loss: 0.6904671604271547\n",
      "Loss: 0.6904406067049187\n",
      "Loss: 0.6904140646975141\n",
      "Loss: 0.6903875343863999\n",
      "Loss: 0.6903610157677034\n",
      "Loss: 0.6903345088522028\n",
      "Loss: 0.6903080136213694\n",
      "Loss: 0.6902815300713314\n",
      "Loss: 0.6902550582128593\n",
      "Loss: 0.6902285980274352\n",
      "Loss: 0.6902021495111896\n"
     ]
    }
   ],
   "source": [
    "def run_logistic_regression():\n",
    "    df = pd.read_csv(\"wine.csv\")\n",
    "    df = df.dropna() \n",
    "    \n",
    "    # keep only wines of quality 5 or 6 since they are mostly balanced\n",
    "    # and to show how logistic regression works for binary classification\n",
    "    print(df['quality'].value_counts(), \"\\n\")\n",
    "    df = df[df['quality'].isin(['5', '6'])]\n",
    "    \n",
    "    # map 5 -> 0 and 6 -> 1 for readability\n",
    "    df.loc[df['quality'] == 5, 'quality'] = 0\n",
    "    df.loc[df['quality'] == 6, 'quality'] = 1\n",
    "    df = df[['pH', 'alcohol', 'quality']]\n",
    "    print(df.head(), \"\\n\")\n",
    "    \n",
    "    logistic_regression = Logistic_Regression()\n",
    "    logistic_regression.train(df, 'quality')\n",
    "    \n",
    "run_logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
